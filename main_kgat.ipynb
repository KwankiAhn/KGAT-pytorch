{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20d258b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "from time import time\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from model.KGAT import KGAT\n",
    "from parser.parser_kgat import *\n",
    "from utils.log_helper import *\n",
    "from utils.metrics import *\n",
    "from utils.model_helper import *\n",
    "from data_loader.loader_kgat import DataLoaderKGAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "567ad787",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, Ks, device):\n",
    "    test_batch_size = dataloader.test_batch_size\n",
    "    train_user_dict = dataloader.train_user_dict\n",
    "    test_user_dict = dataloader.test_user_dict\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    user_ids = list(test_user_dict.keys())\n",
    "    user_ids_batches = [user_ids[i: i + test_batch_size] for i in range(0, len(user_ids), test_batch_size)]\n",
    "    user_ids_batches = [torch.LongTensor(d) for d in user_ids_batches]\n",
    "\n",
    "    n_items = dataloader.n_items\n",
    "    item_ids = torch.arange(n_items, dtype=torch.long).to(device)\n",
    "\n",
    "    cf_scores = []\n",
    "    metric_names = ['precision', 'recall', 'ndcg']\n",
    "    metrics_dict = {k: {m: [] for m in metric_names} for k in Ks}\n",
    "\n",
    "    with tqdm(total=len(user_ids_batches), desc='Evaluating Iteration') as pbar:\n",
    "        for batch_user_ids in user_ids_batches:\n",
    "            batch_user_ids = batch_user_ids.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                batch_scores = model(batch_user_ids, item_ids, mode='predict')       # (n_batch_users, n_items)\n",
    "\n",
    "            batch_scores = batch_scores.cpu()\n",
    "            batch_metrics = calc_metrics_at_k(batch_scores, train_user_dict, test_user_dict, batch_user_ids.cpu().numpy(), item_ids.cpu().numpy(), Ks)\n",
    "\n",
    "            cf_scores.append(batch_scores.numpy())\n",
    "            for k in Ks:\n",
    "                for m in metric_names:\n",
    "                    metrics_dict[k][m].append(batch_metrics[k][m])\n",
    "            pbar.update(1)\n",
    "\n",
    "    cf_scores = np.concatenate(cf_scores, axis=0)\n",
    "    for k in Ks:\n",
    "        for m in metric_names:\n",
    "            metrics_dict[k][m] = np.concatenate(metrics_dict[k][m]).mean()\n",
    "    return cf_scores, metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e15a4097",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args):\n",
    "    # seed\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "    log_save_id = create_log_id(args.save_dir)\n",
    "    logging_config(folder=args.save_dir, name='log{:d}'.format(log_save_id), no_console=False)\n",
    "    logging.info(args)\n",
    "\n",
    "    # GPU / CPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # load data\n",
    "    data = DataLoaderKGAT(args, logging)\n",
    "    if args.use_pretrain == 1:\n",
    "        user_pre_embed = torch.tensor(data.user_pre_embed)\n",
    "        item_pre_embed = torch.tensor(data.item_pre_embed)\n",
    "    else:\n",
    "        user_pre_embed, item_pre_embed = None, None\n",
    "\n",
    "    # construct model & optimizer\n",
    "    model = KGAT(args, data.n_users, data.n_entities, data.n_relations, data.A_in, user_pre_embed, item_pre_embed)\n",
    "    if args.use_pretrain == 2:\n",
    "        model = load_model(model, args.pretrain_model_path)\n",
    "\n",
    "    model.to(device)\n",
    "    logging.info(model)\n",
    "\n",
    "    cf_optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "    kg_optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "    # initialize metrics\n",
    "    best_epoch = -1\n",
    "    best_recall = 0\n",
    "\n",
    "    Ks = eval(args.Ks)\n",
    "    k_min = min(Ks)\n",
    "    k_max = max(Ks)\n",
    "\n",
    "    epoch_list = []\n",
    "    metrics_list = {k: {'precision': [], 'recall': [], 'ndcg': []} for k in Ks}\n",
    "\n",
    "    # train model\n",
    "    for epoch in range(1, args.n_epoch + 1):\n",
    "        time0 = time()\n",
    "        model.train()\n",
    "\n",
    "        # train cf\n",
    "        time1 = time()\n",
    "        cf_total_loss = 0\n",
    "        n_cf_batch = data.n_cf_train // data.cf_batch_size + 1\n",
    "\n",
    "        for iter in range(1, n_cf_batch + 1):\n",
    "            time2 = time()\n",
    "            cf_batch_user, cf_batch_pos_item, cf_batch_neg_item = data.generate_cf_batch(data.train_user_dict, data.cf_batch_size)\n",
    "            cf_batch_user = cf_batch_user.to(device)\n",
    "            cf_batch_pos_item = cf_batch_pos_item.to(device)\n",
    "            cf_batch_neg_item = cf_batch_neg_item.to(device)\n",
    "\n",
    "            cf_batch_loss = model(cf_batch_user, cf_batch_pos_item, cf_batch_neg_item, mode='train_cf')\n",
    "\n",
    "            if np.isnan(cf_batch_loss.cpu().detach().numpy()):\n",
    "                logging.info('ERROR (CF Training): Epoch {:04d} Iter {:04d} / {:04d} Loss is nan.'.format(epoch, iter, n_cf_batch))\n",
    "                sys.exit()\n",
    "\n",
    "            cf_batch_loss.backward()\n",
    "            cf_optimizer.step()\n",
    "            cf_optimizer.zero_grad()\n",
    "            cf_total_loss += cf_batch_loss.item()\n",
    "\n",
    "            if (iter % args.cf_print_every) == 0:\n",
    "                logging.info('CF Training: Epoch {:04d} Iter {:04d} / {:04d} | Time {:.1f}s | Iter Loss {:.4f} | Iter Mean Loss {:.4f}'.format(epoch, iter, n_cf_batch, time() - time2, cf_batch_loss.item(), cf_total_loss / iter))\n",
    "        logging.info('CF Training: Epoch {:04d} Total Iter {:04d} | Total Time {:.1f}s | Iter Mean Loss {:.4f}'.format(epoch, n_cf_batch, time() - time1, cf_total_loss / n_cf_batch))\n",
    "\n",
    "        # train kg\n",
    "        time3 = time()\n",
    "        kg_total_loss = 0\n",
    "        n_kg_batch = data.n_kg_train // data.kg_batch_size + 1\n",
    "\n",
    "        for iter in range(1, n_kg_batch + 1):\n",
    "            time4 = time()\n",
    "            kg_batch_head, kg_batch_relation, kg_batch_pos_tail, kg_batch_neg_tail = data.generate_kg_batch(data.train_kg_dict, data.kg_batch_size, data.n_users_entities)\n",
    "            kg_batch_head = kg_batch_head.to(device)\n",
    "            kg_batch_relation = kg_batch_relation.to(device)\n",
    "            kg_batch_pos_tail = kg_batch_pos_tail.to(device)\n",
    "            kg_batch_neg_tail = kg_batch_neg_tail.to(device)\n",
    "\n",
    "            kg_batch_loss = model(kg_batch_head, kg_batch_relation, kg_batch_pos_tail, kg_batch_neg_tail, mode='train_kg')\n",
    "\n",
    "            if np.isnan(kg_batch_loss.cpu().detach().numpy()):\n",
    "                logging.info('ERROR (KG Training): Epoch {:04d} Iter {:04d} / {:04d} Loss is nan.'.format(epoch, iter, n_kg_batch))\n",
    "                sys.exit()\n",
    "\n",
    "            kg_batch_loss.backward()\n",
    "            kg_optimizer.step()\n",
    "            kg_optimizer.zero_grad()\n",
    "            kg_total_loss += kg_batch_loss.item()\n",
    "\n",
    "            if (iter % args.kg_print_every) == 0:\n",
    "                logging.info('KG Training: Epoch {:04d} Iter {:04d} / {:04d} | Time {:.1f}s | Iter Loss {:.4f} | Iter Mean Loss {:.4f}'.format(epoch, iter, n_kg_batch, time() - time4, kg_batch_loss.item(), kg_total_loss / iter))\n",
    "        logging.info('KG Training: Epoch {:04d} Total Iter {:04d} | Total Time {:.1f}s | Iter Mean Loss {:.4f}'.format(epoch, n_kg_batch, time() - time3, kg_total_loss / n_kg_batch))\n",
    "\n",
    "        # update attention\n",
    "        time5 = time()\n",
    "        h_list = data.h_list.to(device)\n",
    "        t_list = data.t_list.to(device)\n",
    "        r_list = data.r_list.to(device)\n",
    "        relations = list(data.laplacian_dict.keys())\n",
    "        model(h_list, t_list, r_list, relations, mode='update_att')\n",
    "        logging.info('Update Attention: Epoch {:04d} | Total Time {:.1f}s'.format(epoch, time() - time5))\n",
    "\n",
    "        logging.info('CF + KG Training: Epoch {:04d} | Total Time {:.1f}s'.format(epoch, time() - time0))\n",
    "\n",
    "        # evaluate cf\n",
    "        if (epoch % args.evaluate_every) == 0 or epoch == args.n_epoch:\n",
    "            time6 = time()\n",
    "            _, metrics_dict = evaluate(model, data, Ks, device)\n",
    "            logging.info('CF Evaluation: Epoch {:04d} | Total Time {:.1f}s | Precision [{:.4f}, {:.4f}], Recall [{:.4f}, {:.4f}], NDCG [{:.4f}, {:.4f}]'.format(\n",
    "                epoch, time() - time6, metrics_dict[k_min]['precision'], metrics_dict[k_max]['precision'], metrics_dict[k_min]['recall'], metrics_dict[k_max]['recall'], metrics_dict[k_min]['ndcg'], metrics_dict[k_max]['ndcg']))\n",
    "\n",
    "            epoch_list.append(epoch)\n",
    "            for k in Ks:\n",
    "                for m in ['precision', 'recall', 'ndcg']:\n",
    "                    metrics_list[k][m].append(metrics_dict[k][m])\n",
    "            best_recall, should_stop = early_stopping(metrics_list[k_min]['recall'], args.stopping_steps)\n",
    "\n",
    "            if should_stop:\n",
    "                break\n",
    "\n",
    "            if metrics_list[k_min]['recall'].index(best_recall) == len(epoch_list) - 1:\n",
    "                save_model(model, args.save_dir, epoch, best_epoch)\n",
    "                logging.info('Save model on epoch {:04d}!'.format(epoch))\n",
    "                best_epoch = epoch\n",
    "\n",
    "    # save metrics\n",
    "    metrics_df = [epoch_list]\n",
    "    metrics_cols = ['epoch_idx']\n",
    "    for k in Ks:\n",
    "        for m in ['precision', 'recall', 'ndcg']:\n",
    "            metrics_df.append(metrics_list[k][m])\n",
    "            metrics_cols.append('{}@{}'.format(m, k))\n",
    "    metrics_df = pd.DataFrame(metrics_df).transpose()\n",
    "    metrics_df.columns = metrics_cols\n",
    "    metrics_df.to_csv(args.save_dir + '/metrics.tsv', sep='\\t', index=False)\n",
    "\n",
    "    # print best metrics\n",
    "    best_metrics = metrics_df.loc[metrics_df['epoch_idx'] == best_epoch].iloc[0].to_dict()\n",
    "    logging.info('Best CF Evaluation: Epoch {:04d} | Precision [{:.4f}, {:.4f}], Recall [{:.4f}, {:.4f}], NDCG [{:.4f}, {:.4f}]'.format(\n",
    "        int(best_metrics['epoch_idx']), best_metrics['precision@{}'.format(k_min)], best_metrics['precision@{}'.format(k_max)], best_metrics['recall@{}'.format(k_min)], best_metrics['recall@{}'.format(k_max)], best_metrics['ndcg@{}'.format(k_min)], best_metrics['ndcg@{}'.format(k_max)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2f623af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(args):\n",
    "    # GPU / CPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # load data\n",
    "    data = DataLoaderKGAT(args, logging)\n",
    "\n",
    "    # load model\n",
    "    model = KGAT(args, data.n_users, data.n_entities, data.n_relations)\n",
    "    model = load_model(model, args.pretrain_model_path)\n",
    "    model.to(device)\n",
    "\n",
    "    # predict\n",
    "    Ks = eval(args.Ks)\n",
    "    k_min = min(Ks)\n",
    "    k_max = max(Ks)\n",
    "\n",
    "    cf_scores, metrics_dict = evaluate(model, data, Ks, device)\n",
    "    np.save(args.save_dir + 'cf_scores.npy', cf_scores)\n",
    "    print('CF Evaluation: Precision [{:.4f}, {:.4f}], Recall [{:.4f}, {:.4f}], NDCG [{:.4f}, {:.4f}]'.format(\n",
    "        metrics_dict[k_min]['precision'], metrics_dict[k_max]['precision'], metrics_dict[k_min]['recall'], metrics_dict[k_max]['recall'], metrics_dict[k_min]['ndcg'], metrics_dict[k_max]['ndcg']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcef0c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-17 11:00:13,376 - root - INFO - Namespace(seed=2019, data_name='amazon-book', data_dir='datasets/', use_pretrain=1, pretrain_embedding_dir='datasets/pretrain/', pretrain_model_path='trained_model/model.pth', cf_batch_size=1024, kg_batch_size=2048, test_batch_size=10000, embed_dim=64, relation_dim=64, laplacian_type='random-walk', aggregation_type='bi-interaction', conv_dim_list='[64, 32, 16]', mess_dropout='[0.1, 0.1, 0.1]', kg_l2loss_lambda=1e-05, cf_l2loss_lambda=1e-05, lr=0.0001, n_epoch=1000, stopping_steps=10, cf_print_every=1, kg_print_every=1, evaluate_every=10, Ks='[20, 40, 60, 80, 100]', save_dir='trained_model/KGAT/amazon-book/embed-dim64_relation-dim64_random-walk_bi-interaction_64-32-16_lr0.0001_pretrain1/')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All logs will be saved to trained_model/KGAT/amazon-book/embed-dim64_relation-dim64_random-walk_bi-interaction_64-32-16_lr0.0001_pretrain1/log2.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-17 11:01:38,295 - root - INFO - n_users:           70679\n",
      "2022-09-17 11:01:38,297 - root - INFO - n_items:           24915\n",
      "2022-09-17 11:01:38,298 - root - INFO - n_entities:        113487\n",
      "2022-09-17 11:01:38,298 - root - INFO - n_users_entities:  184166\n",
      "2022-09-17 11:01:38,298 - root - INFO - n_relations:       80\n",
      "2022-09-17 11:01:38,299 - root - INFO - n_h_list:          6420520\n",
      "2022-09-17 11:01:38,300 - root - INFO - n_t_list:          6420520\n",
      "2022-09-17 11:01:38,300 - root - INFO - n_r_list:          6420520\n",
      "2022-09-17 11:01:38,300 - root - INFO - n_cf_train:        652514\n",
      "2022-09-17 11:01:38,300 - root - INFO - n_cf_test:         193920\n",
      "2022-09-17 11:01:38,300 - root - INFO - n_kg_train:        6420520\n",
      "/Users/ahnkwanki/src-github/KGAT-pytorch/data_loader/loader_kgat.py:118: RuntimeWarning: divide by zero encountered in power\n",
      "  d_inv = np.power(rowsum, -1.0).flatten()\n",
      "2022-09-17 11:01:41,735 - root - INFO - KGAT(\n",
      "  (entity_user_embed): Embedding(184166, 64)\n",
      "  (relation_embed): Embedding(80, 64)\n",
      "  (aggregator_layers): ModuleList(\n",
      "    (0): Aggregator(\n",
      "      (message_dropout): Dropout(p=0.1, inplace=False)\n",
      "      (activation): LeakyReLU(negative_slope=0.01)\n",
      "      (linear1): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (linear2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    )\n",
      "    (1): Aggregator(\n",
      "      (message_dropout): Dropout(p=0.1, inplace=False)\n",
      "      (activation): LeakyReLU(negative_slope=0.01)\n",
      "      (linear1): Linear(in_features=64, out_features=32, bias=True)\n",
      "      (linear2): Linear(in_features=64, out_features=32, bias=True)\n",
      "    )\n",
      "    (2): Aggregator(\n",
      "      (message_dropout): Dropout(p=0.1, inplace=False)\n",
      "      (activation): LeakyReLU(negative_slope=0.01)\n",
      "      (linear1): Linear(in_features=32, out_features=16, bias=True)\n",
      "      (linear2): Linear(in_features=32, out_features=16, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "2022-09-17 11:01:42,795 - root - INFO - CF Training: Epoch 0001 Iter 0001 / 0638 | Time 1.1s | Iter Loss 0.0285 | Iter Mean Loss 0.0285\n",
      "2022-09-17 11:01:43,755 - root - INFO - CF Training: Epoch 0001 Iter 0002 / 0638 | Time 1.0s | Iter Loss 0.0238 | Iter Mean Loss 0.0261\n",
      "2022-09-17 11:01:44,817 - root - INFO - CF Training: Epoch 0001 Iter 0003 / 0638 | Time 1.1s | Iter Loss 0.0264 | Iter Mean Loss 0.0262\n",
      "2022-09-17 11:01:45,813 - root - INFO - CF Training: Epoch 0001 Iter 0004 / 0638 | Time 1.0s | Iter Loss 0.0224 | Iter Mean Loss 0.0253\n",
      "2022-09-17 11:01:46,680 - root - INFO - CF Training: Epoch 0001 Iter 0005 / 0638 | Time 0.9s | Iter Loss 0.0250 | Iter Mean Loss 0.0252\n",
      "2022-09-17 11:01:47,529 - root - INFO - CF Training: Epoch 0001 Iter 0006 / 0638 | Time 0.8s | Iter Loss 0.0246 | Iter Mean Loss 0.0251\n",
      "2022-09-17 11:01:48,372 - root - INFO - CF Training: Epoch 0001 Iter 0007 / 0638 | Time 0.8s | Iter Loss 0.0233 | Iter Mean Loss 0.0249\n",
      "2022-09-17 11:01:49,209 - root - INFO - CF Training: Epoch 0001 Iter 0008 / 0638 | Time 0.8s | Iter Loss 0.0239 | Iter Mean Loss 0.0247\n",
      "2022-09-17 11:01:50,061 - root - INFO - CF Training: Epoch 0001 Iter 0009 / 0638 | Time 0.9s | Iter Loss 0.0239 | Iter Mean Loss 0.0246\n",
      "2022-09-17 11:01:50,898 - root - INFO - CF Training: Epoch 0001 Iter 0010 / 0638 | Time 0.8s | Iter Loss 0.0225 | Iter Mean Loss 0.0244\n",
      "2022-09-17 11:01:51,771 - root - INFO - CF Training: Epoch 0001 Iter 0011 / 0638 | Time 0.9s | Iter Loss 0.0258 | Iter Mean Loss 0.0245\n",
      "2022-09-17 11:01:52,619 - root - INFO - CF Training: Epoch 0001 Iter 0012 / 0638 | Time 0.8s | Iter Loss 0.0245 | Iter Mean Loss 0.0245\n",
      "2022-09-17 11:01:53,461 - root - INFO - CF Training: Epoch 0001 Iter 0013 / 0638 | Time 0.8s | Iter Loss 0.0287 | Iter Mean Loss 0.0249\n",
      "2022-09-17 11:01:54,311 - root - INFO - CF Training: Epoch 0001 Iter 0014 / 0638 | Time 0.8s | Iter Loss 0.0247 | Iter Mean Loss 0.0248\n",
      "2022-09-17 11:01:55,293 - root - INFO - CF Training: Epoch 0001 Iter 0015 / 0638 | Time 1.0s | Iter Loss 0.0282 | Iter Mean Loss 0.0251\n",
      "2022-09-17 11:01:56,215 - root - INFO - CF Training: Epoch 0001 Iter 0016 / 0638 | Time 0.9s | Iter Loss 0.0253 | Iter Mean Loss 0.0251\n",
      "2022-09-17 11:01:57,063 - root - INFO - CF Training: Epoch 0001 Iter 0017 / 0638 | Time 0.8s | Iter Loss 0.0286 | Iter Mean Loss 0.0253\n",
      "2022-09-17 11:01:57,900 - root - INFO - CF Training: Epoch 0001 Iter 0018 / 0638 | Time 0.8s | Iter Loss 0.0250 | Iter Mean Loss 0.0253\n",
      "2022-09-17 11:01:58,760 - root - INFO - CF Training: Epoch 0001 Iter 0019 / 0638 | Time 0.9s | Iter Loss 0.0284 | Iter Mean Loss 0.0254\n",
      "2022-09-17 11:01:59,610 - root - INFO - CF Training: Epoch 0001 Iter 0020 / 0638 | Time 0.8s | Iter Loss 0.0266 | Iter Mean Loss 0.0255\n",
      "2022-09-17 11:02:00,452 - root - INFO - CF Training: Epoch 0001 Iter 0021 / 0638 | Time 0.8s | Iter Loss 0.0262 | Iter Mean Loss 0.0255\n",
      "2022-09-17 11:02:01,305 - root - INFO - CF Training: Epoch 0001 Iter 0022 / 0638 | Time 0.9s | Iter Loss 0.0291 | Iter Mean Loss 0.0257\n",
      "2022-09-17 11:02:02,141 - root - INFO - CF Training: Epoch 0001 Iter 0023 / 0638 | Time 0.8s | Iter Loss 0.0244 | Iter Mean Loss 0.0256\n",
      "2022-09-17 11:02:02,955 - root - INFO - CF Training: Epoch 0001 Iter 0024 / 0638 | Time 0.8s | Iter Loss 0.0237 | Iter Mean Loss 0.0256\n",
      "2022-09-17 11:02:03,839 - root - INFO - CF Training: Epoch 0001 Iter 0025 / 0638 | Time 0.9s | Iter Loss 0.0257 | Iter Mean Loss 0.0256\n",
      "2022-09-17 11:02:04,685 - root - INFO - CF Training: Epoch 0001 Iter 0026 / 0638 | Time 0.8s | Iter Loss 0.0235 | Iter Mean Loss 0.0255\n",
      "2022-09-17 11:02:05,536 - root - INFO - CF Training: Epoch 0001 Iter 0027 / 0638 | Time 0.9s | Iter Loss 0.0239 | Iter Mean Loss 0.0254\n",
      "2022-09-17 11:02:06,380 - root - INFO - CF Training: Epoch 0001 Iter 0028 / 0638 | Time 0.8s | Iter Loss 0.0250 | Iter Mean Loss 0.0254\n",
      "2022-09-17 11:02:07,271 - root - INFO - CF Training: Epoch 0001 Iter 0029 / 0638 | Time 0.9s | Iter Loss 0.0266 | Iter Mean Loss 0.0254\n",
      "2022-09-17 11:02:08,142 - root - INFO - CF Training: Epoch 0001 Iter 0030 / 0638 | Time 0.9s | Iter Loss 0.0265 | Iter Mean Loss 0.0255\n",
      "2022-09-17 11:02:08,996 - root - INFO - CF Training: Epoch 0001 Iter 0031 / 0638 | Time 0.9s | Iter Loss 0.0274 | Iter Mean Loss 0.0255\n",
      "2022-09-17 11:02:09,862 - root - INFO - CF Training: Epoch 0001 Iter 0032 / 0638 | Time 0.9s | Iter Loss 0.0221 | Iter Mean Loss 0.0254\n",
      "2022-09-17 11:02:10,713 - root - INFO - CF Training: Epoch 0001 Iter 0033 / 0638 | Time 0.9s | Iter Loss 0.0309 | Iter Mean Loss 0.0256\n",
      "2022-09-17 11:02:11,579 - root - INFO - CF Training: Epoch 0001 Iter 0034 / 0638 | Time 0.9s | Iter Loss 0.0290 | Iter Mean Loss 0.0257\n",
      "2022-09-17 11:02:12,433 - root - INFO - CF Training: Epoch 0001 Iter 0035 / 0638 | Time 0.9s | Iter Loss 0.0241 | Iter Mean Loss 0.0257\n",
      "2022-09-17 11:02:13,255 - root - INFO - CF Training: Epoch 0001 Iter 0036 / 0638 | Time 0.8s | Iter Loss 0.0243 | Iter Mean Loss 0.0256\n",
      "2022-09-17 11:02:14,087 - root - INFO - CF Training: Epoch 0001 Iter 0037 / 0638 | Time 0.8s | Iter Loss 0.0255 | Iter Mean Loss 0.0256\n",
      "2022-09-17 11:02:14,936 - root - INFO - CF Training: Epoch 0001 Iter 0038 / 0638 | Time 0.8s | Iter Loss 0.0268 | Iter Mean Loss 0.0256\n",
      "2022-09-17 11:02:15,750 - root - INFO - CF Training: Epoch 0001 Iter 0039 / 0638 | Time 0.8s | Iter Loss 0.0263 | Iter Mean Loss 0.0257\n",
      "2022-09-17 11:02:16,557 - root - INFO - CF Training: Epoch 0001 Iter 0040 / 0638 | Time 0.8s | Iter Loss 0.0270 | Iter Mean Loss 0.0257\n",
      "2022-09-17 11:02:17,372 - root - INFO - CF Training: Epoch 0001 Iter 0041 / 0638 | Time 0.8s | Iter Loss 0.0292 | Iter Mean Loss 0.0258\n",
      "2022-09-17 11:02:18,181 - root - INFO - CF Training: Epoch 0001 Iter 0042 / 0638 | Time 0.8s | Iter Loss 0.0292 | Iter Mean Loss 0.0259\n",
      "2022-09-17 11:02:19,019 - root - INFO - CF Training: Epoch 0001 Iter 0043 / 0638 | Time 0.8s | Iter Loss 0.0254 | Iter Mean Loss 0.0259\n",
      "2022-09-17 11:02:19,878 - root - INFO - CF Training: Epoch 0001 Iter 0044 / 0638 | Time 0.9s | Iter Loss 0.0268 | Iter Mean Loss 0.0259\n",
      "2022-09-17 11:02:20,690 - root - INFO - CF Training: Epoch 0001 Iter 0045 / 0638 | Time 0.8s | Iter Loss 0.0260 | Iter Mean Loss 0.0259\n",
      "2022-09-17 11:02:21,519 - root - INFO - CF Training: Epoch 0001 Iter 0046 / 0638 | Time 0.8s | Iter Loss 0.0327 | Iter Mean Loss 0.0260\n",
      "2022-09-17 11:02:22,389 - root - INFO - CF Training: Epoch 0001 Iter 0047 / 0638 | Time 0.9s | Iter Loss 0.0262 | Iter Mean Loss 0.0260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-17 11:02:23,216 - root - INFO - CF Training: Epoch 0001 Iter 0048 / 0638 | Time 0.8s | Iter Loss 0.0222 | Iter Mean Loss 0.0259\n",
      "2022-09-17 11:02:24,027 - root - INFO - CF Training: Epoch 0001 Iter 0049 / 0638 | Time 0.8s | Iter Loss 0.0234 | Iter Mean Loss 0.0259\n",
      "2022-09-17 11:02:25,167 - root - INFO - CF Training: Epoch 0001 Iter 0050 / 0638 | Time 1.1s | Iter Loss 0.0256 | Iter Mean Loss 0.0259\n",
      "2022-09-17 11:02:26,312 - root - INFO - CF Training: Epoch 0001 Iter 0051 / 0638 | Time 1.1s | Iter Loss 0.0230 | Iter Mean Loss 0.0258\n",
      "2022-09-17 11:02:27,346 - root - INFO - CF Training: Epoch 0001 Iter 0052 / 0638 | Time 1.0s | Iter Loss 0.0260 | Iter Mean Loss 0.0258\n",
      "2022-09-17 11:02:28,419 - root - INFO - CF Training: Epoch 0001 Iter 0053 / 0638 | Time 1.1s | Iter Loss 0.0261 | Iter Mean Loss 0.0258\n",
      "2022-09-17 11:02:29,526 - root - INFO - CF Training: Epoch 0001 Iter 0054 / 0638 | Time 1.1s | Iter Loss 0.0228 | Iter Mean Loss 0.0258\n",
      "2022-09-17 11:02:31,703 - root - INFO - CF Training: Epoch 0001 Iter 0055 / 0638 | Time 2.2s | Iter Loss 0.0269 | Iter Mean Loss 0.0258\n",
      "2022-09-17 11:02:33,416 - root - INFO - CF Training: Epoch 0001 Iter 0056 / 0638 | Time 1.7s | Iter Loss 0.0275 | Iter Mean Loss 0.0258\n",
      "2022-09-17 11:02:34,553 - root - INFO - CF Training: Epoch 0001 Iter 0057 / 0638 | Time 1.1s | Iter Loss 0.0224 | Iter Mean Loss 0.0258\n",
      "2022-09-17 11:02:35,490 - root - INFO - CF Training: Epoch 0001 Iter 0058 / 0638 | Time 0.9s | Iter Loss 0.0229 | Iter Mean Loss 0.0257\n",
      "2022-09-17 11:02:36,393 - root - INFO - CF Training: Epoch 0001 Iter 0059 / 0638 | Time 0.9s | Iter Loss 0.0248 | Iter Mean Loss 0.0257\n",
      "2022-09-17 11:02:37,421 - root - INFO - CF Training: Epoch 0001 Iter 0060 / 0638 | Time 1.0s | Iter Loss 0.0248 | Iter Mean Loss 0.0257\n",
      "2022-09-17 11:02:38,438 - root - INFO - CF Training: Epoch 0001 Iter 0061 / 0638 | Time 1.0s | Iter Loss 0.0253 | Iter Mean Loss 0.0257\n",
      "2022-09-17 11:02:39,392 - root - INFO - CF Training: Epoch 0001 Iter 0062 / 0638 | Time 1.0s | Iter Loss 0.0238 | Iter Mean Loss 0.0257\n",
      "2022-09-17 11:02:40,735 - root - INFO - CF Training: Epoch 0001 Iter 0063 / 0638 | Time 1.3s | Iter Loss 0.0234 | Iter Mean Loss 0.0256\n",
      "2022-09-17 11:02:41,651 - root - INFO - CF Training: Epoch 0001 Iter 0064 / 0638 | Time 0.9s | Iter Loss 0.0244 | Iter Mean Loss 0.0256\n",
      "2022-09-17 11:02:42,497 - root - INFO - CF Training: Epoch 0001 Iter 0065 / 0638 | Time 0.8s | Iter Loss 0.0306 | Iter Mean Loss 0.0257\n",
      "2022-09-17 11:02:43,336 - root - INFO - CF Training: Epoch 0001 Iter 0066 / 0638 | Time 0.8s | Iter Loss 0.0267 | Iter Mean Loss 0.0257\n",
      "2022-09-17 11:02:44,179 - root - INFO - CF Training: Epoch 0001 Iter 0067 / 0638 | Time 0.8s | Iter Loss 0.0243 | Iter Mean Loss 0.0257\n",
      "2022-09-17 11:02:44,994 - root - INFO - CF Training: Epoch 0001 Iter 0068 / 0638 | Time 0.8s | Iter Loss 0.0225 | Iter Mean Loss 0.0256\n",
      "2022-09-17 11:02:45,804 - root - INFO - CF Training: Epoch 0001 Iter 0069 / 0638 | Time 0.8s | Iter Loss 0.0261 | Iter Mean Loss 0.0256\n",
      "2022-09-17 11:02:46,613 - root - INFO - CF Training: Epoch 0001 Iter 0070 / 0638 | Time 0.8s | Iter Loss 0.0226 | Iter Mean Loss 0.0256\n",
      "2022-09-17 11:02:47,422 - root - INFO - CF Training: Epoch 0001 Iter 0071 / 0638 | Time 0.8s | Iter Loss 0.0248 | Iter Mean Loss 0.0256\n",
      "2022-09-17 11:02:48,271 - root - INFO - CF Training: Epoch 0001 Iter 0072 / 0638 | Time 0.8s | Iter Loss 0.0282 | Iter Mean Loss 0.0256\n",
      "2022-09-17 11:02:49,178 - root - INFO - CF Training: Epoch 0001 Iter 0073 / 0638 | Time 0.9s | Iter Loss 0.0307 | Iter Mean Loss 0.0257\n",
      "2022-09-17 11:02:50,053 - root - INFO - CF Training: Epoch 0001 Iter 0074 / 0638 | Time 0.9s | Iter Loss 0.0223 | Iter Mean Loss 0.0256\n",
      "2022-09-17 11:02:50,921 - root - INFO - CF Training: Epoch 0001 Iter 0075 / 0638 | Time 0.9s | Iter Loss 0.0281 | Iter Mean Loss 0.0257\n",
      "2022-09-17 11:02:51,785 - root - INFO - CF Training: Epoch 0001 Iter 0076 / 0638 | Time 0.9s | Iter Loss 0.0228 | Iter Mean Loss 0.0256\n",
      "2022-09-17 11:02:52,743 - root - INFO - CF Training: Epoch 0001 Iter 0077 / 0638 | Time 1.0s | Iter Loss 0.0295 | Iter Mean Loss 0.0257\n",
      "2022-09-17 11:02:53,636 - root - INFO - CF Training: Epoch 0001 Iter 0078 / 0638 | Time 0.9s | Iter Loss 0.0277 | Iter Mean Loss 0.0257\n",
      "2022-09-17 11:02:54,490 - root - INFO - CF Training: Epoch 0001 Iter 0079 / 0638 | Time 0.9s | Iter Loss 0.0264 | Iter Mean Loss 0.0257\n",
      "2022-09-17 11:02:55,376 - root - INFO - CF Training: Epoch 0001 Iter 0080 / 0638 | Time 0.9s | Iter Loss 0.0243 | Iter Mean Loss 0.0257\n",
      "2022-09-17 11:02:56,260 - root - INFO - CF Training: Epoch 0001 Iter 0081 / 0638 | Time 0.9s | Iter Loss 0.0271 | Iter Mean Loss 0.0257\n",
      "2022-09-17 11:02:57,140 - root - INFO - CF Training: Epoch 0001 Iter 0082 / 0638 | Time 0.9s | Iter Loss 0.0211 | Iter Mean Loss 0.0257\n",
      "2022-09-17 11:02:58,030 - root - INFO - CF Training: Epoch 0001 Iter 0083 / 0638 | Time 0.9s | Iter Loss 0.0261 | Iter Mean Loss 0.0257\n",
      "2022-09-17 11:02:58,895 - root - INFO - CF Training: Epoch 0001 Iter 0084 / 0638 | Time 0.9s | Iter Loss 0.0241 | Iter Mean Loss 0.0256\n",
      "2022-09-17 11:02:59,756 - root - INFO - CF Training: Epoch 0001 Iter 0085 / 0638 | Time 0.9s | Iter Loss 0.0228 | Iter Mean Loss 0.0256\n",
      "2022-09-17 11:03:00,623 - root - INFO - CF Training: Epoch 0001 Iter 0086 / 0638 | Time 0.9s | Iter Loss 0.0235 | Iter Mean Loss 0.0256\n",
      "2022-09-17 11:03:01,482 - root - INFO - CF Training: Epoch 0001 Iter 0087 / 0638 | Time 0.9s | Iter Loss 0.0235 | Iter Mean Loss 0.0256\n",
      "2022-09-17 11:03:02,388 - root - INFO - CF Training: Epoch 0001 Iter 0088 / 0638 | Time 0.9s | Iter Loss 0.0233 | Iter Mean Loss 0.0255\n",
      "2022-09-17 11:03:03,303 - root - INFO - CF Training: Epoch 0001 Iter 0089 / 0638 | Time 0.9s | Iter Loss 0.0259 | Iter Mean Loss 0.0255\n",
      "2022-09-17 11:03:04,261 - root - INFO - CF Training: Epoch 0001 Iter 0090 / 0638 | Time 1.0s | Iter Loss 0.0296 | Iter Mean Loss 0.0256\n",
      "2022-09-17 11:03:05,123 - root - INFO - CF Training: Epoch 0001 Iter 0091 / 0638 | Time 0.9s | Iter Loss 0.0247 | Iter Mean Loss 0.0256\n",
      "2022-09-17 11:03:05,969 - root - INFO - CF Training: Epoch 0001 Iter 0092 / 0638 | Time 0.8s | Iter Loss 0.0233 | Iter Mean Loss 0.0256\n",
      "2022-09-17 11:03:06,820 - root - INFO - CF Training: Epoch 0001 Iter 0093 / 0638 | Time 0.9s | Iter Loss 0.0225 | Iter Mean Loss 0.0255\n",
      "2022-09-17 11:03:07,670 - root - INFO - CF Training: Epoch 0001 Iter 0094 / 0638 | Time 0.8s | Iter Loss 0.0275 | Iter Mean Loss 0.0255\n",
      "2022-09-17 11:03:08,513 - root - INFO - CF Training: Epoch 0001 Iter 0095 / 0638 | Time 0.8s | Iter Loss 0.0275 | Iter Mean Loss 0.0256\n",
      "2022-09-17 11:03:09,326 - root - INFO - CF Training: Epoch 0001 Iter 0096 / 0638 | Time 0.8s | Iter Loss 0.0273 | Iter Mean Loss 0.0256\n",
      "2022-09-17 11:03:10,154 - root - INFO - CF Training: Epoch 0001 Iter 0097 / 0638 | Time 0.8s | Iter Loss 0.0242 | Iter Mean Loss 0.0256\n",
      "2022-09-17 11:03:10,968 - root - INFO - CF Training: Epoch 0001 Iter 0098 / 0638 | Time 0.8s | Iter Loss 0.0244 | Iter Mean Loss 0.0256\n",
      "2022-09-17 11:03:11,793 - root - INFO - CF Training: Epoch 0001 Iter 0099 / 0638 | Time 0.8s | Iter Loss 0.0256 | Iter Mean Loss 0.0256\n",
      "2022-09-17 11:03:12,721 - root - INFO - CF Training: Epoch 0001 Iter 0100 / 0638 | Time 0.9s | Iter Loss 0.0227 | Iter Mean Loss 0.0255\n",
      "2022-09-17 11:03:13,996 - root - INFO - CF Training: Epoch 0001 Iter 0101 / 0638 | Time 1.3s | Iter Loss 0.0282 | Iter Mean Loss 0.0255\n",
      "2022-09-17 11:03:15,029 - root - INFO - CF Training: Epoch 0001 Iter 0102 / 0638 | Time 1.0s | Iter Loss 0.0259 | Iter Mean Loss 0.0256\n",
      "2022-09-17 11:03:15,929 - root - INFO - CF Training: Epoch 0001 Iter 0103 / 0638 | Time 0.9s | Iter Loss 0.0311 | Iter Mean Loss 0.0256\n",
      "2022-09-17 11:03:16,789 - root - INFO - CF Training: Epoch 0001 Iter 0104 / 0638 | Time 0.9s | Iter Loss 0.0249 | Iter Mean Loss 0.0256\n",
      "2022-09-17 11:03:17,784 - root - INFO - CF Training: Epoch 0001 Iter 0105 / 0638 | Time 1.0s | Iter Loss 0.0254 | Iter Mean Loss 0.0256\n",
      "2022-09-17 11:03:18,745 - root - INFO - CF Training: Epoch 0001 Iter 0106 / 0638 | Time 1.0s | Iter Loss 0.0243 | Iter Mean Loss 0.0256\n",
      "2022-09-17 11:03:20,082 - root - INFO - CF Training: Epoch 0001 Iter 0107 / 0638 | Time 1.3s | Iter Loss 0.0248 | Iter Mean Loss 0.0256\n",
      "2022-09-17 11:03:21,229 - root - INFO - CF Training: Epoch 0001 Iter 0108 / 0638 | Time 1.1s | Iter Loss 0.0233 | Iter Mean Loss 0.0256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-17 11:03:22,218 - root - INFO - CF Training: Epoch 0001 Iter 0109 / 0638 | Time 1.0s | Iter Loss 0.0223 | Iter Mean Loss 0.0255\n",
      "2022-09-17 11:03:23,189 - root - INFO - CF Training: Epoch 0001 Iter 0110 / 0638 | Time 1.0s | Iter Loss 0.0273 | Iter Mean Loss 0.0255\n",
      "2022-09-17 11:03:25,347 - root - INFO - CF Training: Epoch 0001 Iter 0111 / 0638 | Time 2.2s | Iter Loss 0.0228 | Iter Mean Loss 0.0255\n",
      "2022-09-17 11:03:26,697 - root - INFO - CF Training: Epoch 0001 Iter 0112 / 0638 | Time 1.3s | Iter Loss 0.0231 | Iter Mean Loss 0.0255\n",
      "2022-09-17 11:03:27,609 - root - INFO - CF Training: Epoch 0001 Iter 0113 / 0638 | Time 0.9s | Iter Loss 0.0246 | Iter Mean Loss 0.0255\n",
      "2022-09-17 11:03:28,486 - root - INFO - CF Training: Epoch 0001 Iter 0114 / 0638 | Time 0.9s | Iter Loss 0.0276 | Iter Mean Loss 0.0255\n",
      "2022-09-17 11:03:29,328 - root - INFO - CF Training: Epoch 0001 Iter 0115 / 0638 | Time 0.8s | Iter Loss 0.0257 | Iter Mean Loss 0.0255\n",
      "2022-09-17 11:03:30,237 - root - INFO - CF Training: Epoch 0001 Iter 0116 / 0638 | Time 0.9s | Iter Loss 0.0243 | Iter Mean Loss 0.0255\n",
      "2022-09-17 11:03:31,073 - root - INFO - CF Training: Epoch 0001 Iter 0117 / 0638 | Time 0.8s | Iter Loss 0.0220 | Iter Mean Loss 0.0255\n",
      "2022-09-17 11:03:32,090 - root - INFO - CF Training: Epoch 0001 Iter 0118 / 0638 | Time 1.0s | Iter Loss 0.0239 | Iter Mean Loss 0.0255\n",
      "2022-09-17 11:03:33,064 - root - INFO - CF Training: Epoch 0001 Iter 0119 / 0638 | Time 1.0s | Iter Loss 0.0289 | Iter Mean Loss 0.0255\n",
      "2022-09-17 11:03:33,952 - root - INFO - CF Training: Epoch 0001 Iter 0120 / 0638 | Time 0.9s | Iter Loss 0.0268 | Iter Mean Loss 0.0255\n",
      "2022-09-17 11:03:34,813 - root - INFO - CF Training: Epoch 0001 Iter 0121 / 0638 | Time 0.9s | Iter Loss 0.0268 | Iter Mean Loss 0.0255\n",
      "2022-09-17 11:03:35,658 - root - INFO - CF Training: Epoch 0001 Iter 0122 / 0638 | Time 0.8s | Iter Loss 0.0231 | Iter Mean Loss 0.0255\n",
      "2022-09-17 11:03:36,505 - root - INFO - CF Training: Epoch 0001 Iter 0123 / 0638 | Time 0.8s | Iter Loss 0.0212 | Iter Mean Loss 0.0254\n",
      "2022-09-17 11:03:37,406 - root - INFO - CF Training: Epoch 0001 Iter 0124 / 0638 | Time 0.9s | Iter Loss 0.0239 | Iter Mean Loss 0.0254\n",
      "2022-09-17 11:03:38,287 - root - INFO - CF Training: Epoch 0001 Iter 0125 / 0638 | Time 0.9s | Iter Loss 0.0254 | Iter Mean Loss 0.0254\n",
      "2022-09-17 11:03:39,178 - root - INFO - CF Training: Epoch 0001 Iter 0126 / 0638 | Time 0.9s | Iter Loss 0.0243 | Iter Mean Loss 0.0254\n",
      "2022-09-17 11:03:40,069 - root - INFO - CF Training: Epoch 0001 Iter 0127 / 0638 | Time 0.9s | Iter Loss 0.0266 | Iter Mean Loss 0.0254\n",
      "2022-09-17 11:03:40,962 - root - INFO - CF Training: Epoch 0001 Iter 0128 / 0638 | Time 0.9s | Iter Loss 0.0240 | Iter Mean Loss 0.0254\n",
      "2022-09-17 11:03:41,828 - root - INFO - CF Training: Epoch 0001 Iter 0129 / 0638 | Time 0.9s | Iter Loss 0.0277 | Iter Mean Loss 0.0254\n",
      "2022-09-17 11:03:42,696 - root - INFO - CF Training: Epoch 0001 Iter 0130 / 0638 | Time 0.9s | Iter Loss 0.0225 | Iter Mean Loss 0.0254\n",
      "2022-09-17 11:03:43,566 - root - INFO - CF Training: Epoch 0001 Iter 0131 / 0638 | Time 0.9s | Iter Loss 0.0249 | Iter Mean Loss 0.0254\n",
      "2022-09-17 11:03:44,418 - root - INFO - CF Training: Epoch 0001 Iter 0132 / 0638 | Time 0.9s | Iter Loss 0.0205 | Iter Mean Loss 0.0254\n",
      "2022-09-17 11:03:45,341 - root - INFO - CF Training: Epoch 0001 Iter 0133 / 0638 | Time 0.9s | Iter Loss 0.0251 | Iter Mean Loss 0.0254\n",
      "2022-09-17 11:03:46,239 - root - INFO - CF Training: Epoch 0001 Iter 0134 / 0638 | Time 0.9s | Iter Loss 0.0221 | Iter Mean Loss 0.0254\n",
      "2022-09-17 11:03:47,193 - root - INFO - CF Training: Epoch 0001 Iter 0135 / 0638 | Time 1.0s | Iter Loss 0.0277 | Iter Mean Loss 0.0254\n",
      "2022-09-17 11:03:48,074 - root - INFO - CF Training: Epoch 0001 Iter 0136 / 0638 | Time 0.9s | Iter Loss 0.0225 | Iter Mean Loss 0.0253\n",
      "2022-09-17 11:03:48,968 - root - INFO - CF Training: Epoch 0001 Iter 0137 / 0638 | Time 0.9s | Iter Loss 0.0201 | Iter Mean Loss 0.0253\n",
      "2022-09-17 11:03:49,824 - root - INFO - CF Training: Epoch 0001 Iter 0138 / 0638 | Time 0.9s | Iter Loss 0.0273 | Iter Mean Loss 0.0253\n",
      "2022-09-17 11:03:50,802 - root - INFO - CF Training: Epoch 0001 Iter 0139 / 0638 | Time 1.0s | Iter Loss 0.0252 | Iter Mean Loss 0.0253\n",
      "2022-09-17 11:03:51,827 - root - INFO - CF Training: Epoch 0001 Iter 0140 / 0638 | Time 1.0s | Iter Loss 0.0233 | Iter Mean Loss 0.0253\n",
      "2022-09-17 11:03:52,894 - root - INFO - CF Training: Epoch 0001 Iter 0141 / 0638 | Time 1.1s | Iter Loss 0.0263 | Iter Mean Loss 0.0253\n",
      "2022-09-17 11:03:53,911 - root - INFO - CF Training: Epoch 0001 Iter 0142 / 0638 | Time 1.0s | Iter Loss 0.0236 | Iter Mean Loss 0.0253\n",
      "2022-09-17 11:03:54,838 - root - INFO - CF Training: Epoch 0001 Iter 0143 / 0638 | Time 0.9s | Iter Loss 0.0223 | Iter Mean Loss 0.0253\n",
      "2022-09-17 11:03:55,760 - root - INFO - CF Training: Epoch 0001 Iter 0144 / 0638 | Time 0.9s | Iter Loss 0.0242 | Iter Mean Loss 0.0253\n",
      "2022-09-17 11:03:56,626 - root - INFO - CF Training: Epoch 0001 Iter 0145 / 0638 | Time 0.9s | Iter Loss 0.0250 | Iter Mean Loss 0.0253\n",
      "2022-09-17 11:03:57,519 - root - INFO - CF Training: Epoch 0001 Iter 0146 / 0638 | Time 0.9s | Iter Loss 0.0263 | Iter Mean Loss 0.0253\n",
      "2022-09-17 11:03:58,437 - root - INFO - CF Training: Epoch 0001 Iter 0147 / 0638 | Time 0.9s | Iter Loss 0.0251 | Iter Mean Loss 0.0253\n",
      "2022-09-17 11:03:59,373 - root - INFO - CF Training: Epoch 0001 Iter 0148 / 0638 | Time 0.9s | Iter Loss 0.0225 | Iter Mean Loss 0.0253\n",
      "2022-09-17 11:04:00,440 - root - INFO - CF Training: Epoch 0001 Iter 0149 / 0638 | Time 1.1s | Iter Loss 0.0259 | Iter Mean Loss 0.0253\n",
      "2022-09-17 11:04:02,622 - root - INFO - CF Training: Epoch 0001 Iter 0150 / 0638 | Time 2.2s | Iter Loss 0.0216 | Iter Mean Loss 0.0252\n",
      "2022-09-17 11:04:03,759 - root - INFO - CF Training: Epoch 0001 Iter 0151 / 0638 | Time 1.1s | Iter Loss 0.0254 | Iter Mean Loss 0.0252\n",
      "2022-09-17 11:04:04,814 - root - INFO - CF Training: Epoch 0001 Iter 0152 / 0638 | Time 1.1s | Iter Loss 0.0280 | Iter Mean Loss 0.0253\n",
      "2022-09-17 11:04:05,801 - root - INFO - CF Training: Epoch 0001 Iter 0153 / 0638 | Time 1.0s | Iter Loss 0.0227 | Iter Mean Loss 0.0252\n",
      "2022-09-17 11:04:06,803 - root - INFO - CF Training: Epoch 0001 Iter 0154 / 0638 | Time 1.0s | Iter Loss 0.0220 | Iter Mean Loss 0.0252\n",
      "2022-09-17 11:04:07,796 - root - INFO - CF Training: Epoch 0001 Iter 0155 / 0638 | Time 1.0s | Iter Loss 0.0186 | Iter Mean Loss 0.0252\n",
      "2022-09-17 11:04:08,771 - root - INFO - CF Training: Epoch 0001 Iter 0156 / 0638 | Time 1.0s | Iter Loss 0.0252 | Iter Mean Loss 0.0252\n",
      "2022-09-17 11:04:09,671 - root - INFO - CF Training: Epoch 0001 Iter 0157 / 0638 | Time 0.9s | Iter Loss 0.0231 | Iter Mean Loss 0.0252\n",
      "2022-09-17 11:04:10,558 - root - INFO - CF Training: Epoch 0001 Iter 0158 / 0638 | Time 0.9s | Iter Loss 0.0261 | Iter Mean Loss 0.0252\n",
      "2022-09-17 11:04:11,450 - root - INFO - CF Training: Epoch 0001 Iter 0159 / 0638 | Time 0.9s | Iter Loss 0.0259 | Iter Mean Loss 0.0252\n",
      "2022-09-17 11:04:12,310 - root - INFO - CF Training: Epoch 0001 Iter 0160 / 0638 | Time 0.9s | Iter Loss 0.0239 | Iter Mean Loss 0.0252\n",
      "2022-09-17 11:04:13,189 - root - INFO - CF Training: Epoch 0001 Iter 0161 / 0638 | Time 0.9s | Iter Loss 0.0263 | Iter Mean Loss 0.0252\n",
      "2022-09-17 11:04:14,094 - root - INFO - CF Training: Epoch 0001 Iter 0162 / 0638 | Time 0.9s | Iter Loss 0.0256 | Iter Mean Loss 0.0252\n",
      "2022-09-17 11:04:15,105 - root - INFO - CF Training: Epoch 0001 Iter 0163 / 0638 | Time 1.0s | Iter Loss 0.0316 | Iter Mean Loss 0.0252\n",
      "2022-09-17 11:04:16,064 - root - INFO - CF Training: Epoch 0001 Iter 0164 / 0638 | Time 1.0s | Iter Loss 0.0217 | Iter Mean Loss 0.0252\n",
      "2022-09-17 11:04:17,186 - root - INFO - CF Training: Epoch 0001 Iter 0165 / 0638 | Time 1.1s | Iter Loss 0.0227 | Iter Mean Loss 0.0252\n",
      "2022-09-17 11:04:18,615 - root - INFO - CF Training: Epoch 0001 Iter 0166 / 0638 | Time 1.4s | Iter Loss 0.0236 | Iter Mean Loss 0.0252\n",
      "2022-09-17 11:04:19,564 - root - INFO - CF Training: Epoch 0001 Iter 0167 / 0638 | Time 0.9s | Iter Loss 0.0227 | Iter Mean Loss 0.0252\n",
      "2022-09-17 11:04:20,462 - root - INFO - CF Training: Epoch 0001 Iter 0168 / 0638 | Time 0.9s | Iter Loss 0.0272 | Iter Mean Loss 0.0252\n",
      "2022-09-17 11:04:21,318 - root - INFO - CF Training: Epoch 0001 Iter 0169 / 0638 | Time 0.9s | Iter Loss 0.0254 | Iter Mean Loss 0.0252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-17 11:04:22,204 - root - INFO - CF Training: Epoch 0001 Iter 0170 / 0638 | Time 0.9s | Iter Loss 0.0321 | Iter Mean Loss 0.0252\n",
      "2022-09-17 11:04:23,113 - root - INFO - CF Training: Epoch 0001 Iter 0171 / 0638 | Time 0.9s | Iter Loss 0.0222 | Iter Mean Loss 0.0252\n",
      "2022-09-17 11:04:24,034 - root - INFO - CF Training: Epoch 0001 Iter 0172 / 0638 | Time 0.9s | Iter Loss 0.0270 | Iter Mean Loss 0.0252\n",
      "2022-09-17 11:04:25,029 - root - INFO - CF Training: Epoch 0001 Iter 0173 / 0638 | Time 1.0s | Iter Loss 0.0260 | Iter Mean Loss 0.0252\n",
      "2022-09-17 11:04:25,913 - root - INFO - CF Training: Epoch 0001 Iter 0174 / 0638 | Time 0.9s | Iter Loss 0.0215 | Iter Mean Loss 0.0252\n",
      "2022-09-17 11:04:26,818 - root - INFO - CF Training: Epoch 0001 Iter 0175 / 0638 | Time 0.9s | Iter Loss 0.0232 | Iter Mean Loss 0.0252\n",
      "2022-09-17 11:04:27,687 - root - INFO - CF Training: Epoch 0001 Iter 0176 / 0638 | Time 0.9s | Iter Loss 0.0224 | Iter Mean Loss 0.0252\n",
      "2022-09-17 11:04:28,546 - root - INFO - CF Training: Epoch 0001 Iter 0177 / 0638 | Time 0.9s | Iter Loss 0.0256 | Iter Mean Loss 0.0252\n",
      "2022-09-17 11:04:29,641 - root - INFO - CF Training: Epoch 0001 Iter 0178 / 0638 | Time 1.1s | Iter Loss 0.0263 | Iter Mean Loss 0.0252\n",
      "2022-09-17 11:04:30,724 - root - INFO - CF Training: Epoch 0001 Iter 0179 / 0638 | Time 1.1s | Iter Loss 0.0275 | Iter Mean Loss 0.0252\n",
      "2022-09-17 11:04:31,627 - root - INFO - CF Training: Epoch 0001 Iter 0180 / 0638 | Time 0.9s | Iter Loss 0.0240 | Iter Mean Loss 0.0252\n",
      "2022-09-17 11:04:32,558 - root - INFO - CF Training: Epoch 0001 Iter 0181 / 0638 | Time 0.9s | Iter Loss 0.0278 | Iter Mean Loss 0.0252\n",
      "2022-09-17 11:04:33,470 - root - INFO - CF Training: Epoch 0001 Iter 0182 / 0638 | Time 0.9s | Iter Loss 0.0229 | Iter Mean Loss 0.0252\n",
      "2022-09-17 11:04:34,345 - root - INFO - CF Training: Epoch 0001 Iter 0183 / 0638 | Time 0.9s | Iter Loss 0.0237 | Iter Mean Loss 0.0252\n",
      "2022-09-17 11:04:35,195 - root - INFO - CF Training: Epoch 0001 Iter 0184 / 0638 | Time 0.8s | Iter Loss 0.0237 | Iter Mean Loss 0.0252\n",
      "2022-09-17 11:04:36,051 - root - INFO - CF Training: Epoch 0001 Iter 0185 / 0638 | Time 0.9s | Iter Loss 0.0232 | Iter Mean Loss 0.0252\n",
      "2022-09-17 11:04:36,919 - root - INFO - CF Training: Epoch 0001 Iter 0186 / 0638 | Time 0.9s | Iter Loss 0.0235 | Iter Mean Loss 0.0251\n",
      "2022-09-17 11:04:37,784 - root - INFO - CF Training: Epoch 0001 Iter 0187 / 0638 | Time 0.9s | Iter Loss 0.0218 | Iter Mean Loss 0.0251\n",
      "2022-09-17 11:04:38,624 - root - INFO - CF Training: Epoch 0001 Iter 0188 / 0638 | Time 0.8s | Iter Loss 0.0269 | Iter Mean Loss 0.0251\n",
      "2022-09-17 11:04:39,455 - root - INFO - CF Training: Epoch 0001 Iter 0189 / 0638 | Time 0.8s | Iter Loss 0.0210 | Iter Mean Loss 0.0251\n",
      "2022-09-17 11:04:40,292 - root - INFO - CF Training: Epoch 0001 Iter 0190 / 0638 | Time 0.8s | Iter Loss 0.0232 | Iter Mean Loss 0.0251\n",
      "2022-09-17 11:04:41,128 - root - INFO - CF Training: Epoch 0001 Iter 0191 / 0638 | Time 0.8s | Iter Loss 0.0232 | Iter Mean Loss 0.0251\n",
      "2022-09-17 11:04:42,003 - root - INFO - CF Training: Epoch 0001 Iter 0192 / 0638 | Time 0.9s | Iter Loss 0.0249 | Iter Mean Loss 0.0251\n",
      "2022-09-17 11:04:42,896 - root - INFO - CF Training: Epoch 0001 Iter 0193 / 0638 | Time 0.9s | Iter Loss 0.0210 | Iter Mean Loss 0.0251\n",
      "2022-09-17 11:04:43,774 - root - INFO - CF Training: Epoch 0001 Iter 0194 / 0638 | Time 0.9s | Iter Loss 0.0263 | Iter Mean Loss 0.0251\n",
      "2022-09-17 11:04:44,673 - root - INFO - CF Training: Epoch 0001 Iter 0195 / 0638 | Time 0.9s | Iter Loss 0.0224 | Iter Mean Loss 0.0251\n",
      "2022-09-17 11:04:45,583 - root - INFO - CF Training: Epoch 0001 Iter 0196 / 0638 | Time 0.9s | Iter Loss 0.0238 | Iter Mean Loss 0.0251\n",
      "2022-09-17 11:04:46,445 - root - INFO - CF Training: Epoch 0001 Iter 0197 / 0638 | Time 0.9s | Iter Loss 0.0246 | Iter Mean Loss 0.0251\n",
      "2022-09-17 11:04:47,318 - root - INFO - CF Training: Epoch 0001 Iter 0198 / 0638 | Time 0.9s | Iter Loss 0.0255 | Iter Mean Loss 0.0251\n",
      "2022-09-17 11:04:48,185 - root - INFO - CF Training: Epoch 0001 Iter 0199 / 0638 | Time 0.9s | Iter Loss 0.0256 | Iter Mean Loss 0.0251\n",
      "2022-09-17 11:04:49,117 - root - INFO - CF Training: Epoch 0001 Iter 0200 / 0638 | Time 0.9s | Iter Loss 0.0243 | Iter Mean Loss 0.0251\n",
      "2022-09-17 11:04:50,063 - root - INFO - CF Training: Epoch 0001 Iter 0201 / 0638 | Time 0.9s | Iter Loss 0.0216 | Iter Mean Loss 0.0250\n",
      "2022-09-17 11:04:50,974 - root - INFO - CF Training: Epoch 0001 Iter 0202 / 0638 | Time 0.9s | Iter Loss 0.0206 | Iter Mean Loss 0.0250\n",
      "2022-09-17 11:04:51,849 - root - INFO - CF Training: Epoch 0001 Iter 0203 / 0638 | Time 0.9s | Iter Loss 0.0198 | Iter Mean Loss 0.0250\n",
      "2022-09-17 11:04:52,716 - root - INFO - CF Training: Epoch 0001 Iter 0204 / 0638 | Time 0.9s | Iter Loss 0.0247 | Iter Mean Loss 0.0250\n",
      "2022-09-17 11:04:53,574 - root - INFO - CF Training: Epoch 0001 Iter 0205 / 0638 | Time 0.9s | Iter Loss 0.0192 | Iter Mean Loss 0.0250\n",
      "2022-09-17 11:04:54,425 - root - INFO - CF Training: Epoch 0001 Iter 0206 / 0638 | Time 0.9s | Iter Loss 0.0219 | Iter Mean Loss 0.0249\n",
      "2022-09-17 11:04:55,263 - root - INFO - CF Training: Epoch 0001 Iter 0207 / 0638 | Time 0.8s | Iter Loss 0.0256 | Iter Mean Loss 0.0249\n",
      "2022-09-17 11:04:56,100 - root - INFO - CF Training: Epoch 0001 Iter 0208 / 0638 | Time 0.8s | Iter Loss 0.0231 | Iter Mean Loss 0.0249\n",
      "2022-09-17 11:04:56,942 - root - INFO - CF Training: Epoch 0001 Iter 0209 / 0638 | Time 0.8s | Iter Loss 0.0271 | Iter Mean Loss 0.0250\n",
      "2022-09-17 11:04:57,786 - root - INFO - CF Training: Epoch 0001 Iter 0210 / 0638 | Time 0.8s | Iter Loss 0.0255 | Iter Mean Loss 0.0250\n",
      "2022-09-17 11:04:58,628 - root - INFO - CF Training: Epoch 0001 Iter 0211 / 0638 | Time 0.8s | Iter Loss 0.0213 | Iter Mean Loss 0.0249\n",
      "2022-09-17 11:04:59,469 - root - INFO - CF Training: Epoch 0001 Iter 0212 / 0638 | Time 0.8s | Iter Loss 0.0218 | Iter Mean Loss 0.0249\n",
      "2022-09-17 11:05:00,317 - root - INFO - CF Training: Epoch 0001 Iter 0213 / 0638 | Time 0.8s | Iter Loss 0.0211 | Iter Mean Loss 0.0249\n",
      "2022-09-17 11:05:01,182 - root - INFO - CF Training: Epoch 0001 Iter 0214 / 0638 | Time 0.9s | Iter Loss 0.0235 | Iter Mean Loss 0.0249\n",
      "2022-09-17 11:05:02,047 - root - INFO - CF Training: Epoch 0001 Iter 0215 / 0638 | Time 0.9s | Iter Loss 0.0189 | Iter Mean Loss 0.0249\n",
      "2022-09-17 11:05:02,915 - root - INFO - CF Training: Epoch 0001 Iter 0216 / 0638 | Time 0.9s | Iter Loss 0.0295 | Iter Mean Loss 0.0249\n",
      "2022-09-17 11:05:03,767 - root - INFO - CF Training: Epoch 0001 Iter 0217 / 0638 | Time 0.9s | Iter Loss 0.0225 | Iter Mean Loss 0.0249\n",
      "2022-09-17 11:05:04,622 - root - INFO - CF Training: Epoch 0001 Iter 0218 / 0638 | Time 0.9s | Iter Loss 0.0217 | Iter Mean Loss 0.0249\n",
      "2022-09-17 11:05:05,472 - root - INFO - CF Training: Epoch 0001 Iter 0219 / 0638 | Time 0.8s | Iter Loss 0.0231 | Iter Mean Loss 0.0249\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m      2\u001b[0m     args \u001b[38;5;241m=\u001b[39m parse_kgat_args()\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     65\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mERROR (CF Training): Epoch \u001b[39m\u001b[38;5;132;01m{:04d}\u001b[39;00m\u001b[38;5;124m Iter \u001b[39m\u001b[38;5;132;01m{:04d}\u001b[39;00m\u001b[38;5;124m / \u001b[39m\u001b[38;5;132;01m{:04d}\u001b[39;00m\u001b[38;5;124m Loss is nan.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch, \u001b[38;5;28miter\u001b[39m, n_cf_batch))\n\u001b[1;32m     66\u001b[0m     sys\u001b[38;5;241m.\u001b[39mexit()\n\u001b[0;32m---> 68\u001b[0m \u001b[43mcf_batch_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m cf_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     70\u001b[0m cf_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/venv39/lib/python3.9/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv39/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    args = parse_kgat_args()\n",
    "    train(args)\n",
    "    # predict(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3d230d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
